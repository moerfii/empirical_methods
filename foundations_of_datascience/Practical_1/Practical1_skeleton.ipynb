{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "id1w7kwZVF3I"
   },
   "source": [
    "# Practical 1 : Implementation of Linear Regression (Ridge, Lasso)\n",
    "\n",
    "First part:\n",
    "- Implement linear regression model \n",
    "    - using least squares method\n",
    "    - implement directly using the NumPy package\n",
    "\n",
    "Second part:\n",
    "- regularization\n",
    "- polynomial basis expansion\n",
    "- cross validation\n",
    "- scikit-learn: https://scikit-learn.org/\n",
    "\n",
    "You will need to use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CTZv9o5i4gy3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import _pickle as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b1-ZQWqTVPno"
   },
   "source": [
    "For the purpose of testing, we’ll use the winequality dataset. The dataset is available here:\n",
    "https://archive.ics.uci.edu/ml/datasets/Wine+Quality In order to make it easier to import the dataset, we’ve converted the data to the numpy array format and shuffled it so that you can start the practical directly. The dataset is available on the course website. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TzDL9RQiVaPY"
   },
   "source": [
    "The dataset has two files. We’ll focus on the white wine data, which is the larger dataset. You can load the data from the files as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1423,
     "status": "ok",
     "timestamp": 1596436129238,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "NYkwbebUVO_i",
    "outputId": "80ed8916-85c3-4564-cda8-d8a8f36aaa1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X is a matrix with shape (4898, 11), which has 4898 records and 11 attributes.\n",
      "y is a vector with 4898 values, which stores the corresponding labels of the data records in X\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "# X is a matrix such that each row stores a data record \n",
    "# y is a vector of the corresponding labels of the records\n",
    "X, y = cp.load(open('winequality-white.pickle', 'rb'))\n",
    "\n",
    "# check the size of the data\n",
    "print(\"X is a matrix with shape {}, which has {} records and {} attributes.\".format(X.shape, X.shape[0], X.shape[1]))\n",
    "print(\"y is a vector with {} values, which stores the corresponding labels of the data records in X\".format(y.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CGuNg0KbWN0z"
   },
   "source": [
    "In order to get consistent results, all students should use the same 80% of the data as training\n",
    "data. We’ll use the remaining as test data. To achieve this split run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1416,
     "status": "ok",
     "timestamp": 1596436129239,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "6ZqbBa8bWNYg",
    "outputId": "da274c4e-c3ed-4ac0-8442-27befcf26f4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (3918, 11)\n",
      "Shape of y_train: (3918,)\n",
      "Shape of X_test: (980, 11)\n",
      "Shape of y_test: (980,)\n"
     ]
    }
   ],
   "source": [
    "# The function splits the dataset into the training dataset and the test dataset.\n",
    "# The parameter split_coeff is a percentage value such that\n",
    "# the first split_coeff of the dataset goes to the training dataset, \n",
    "# and the remaining data goes to the test dataset.\n",
    "def split_data(X, y, split_coeff):\n",
    "    N, _ = X.shape # get the number of records (rows)\n",
    "    train_size = int(split_coeff * N) # use the first split_coeff of the data as the training data\n",
    "    X_train = X[:train_size] # the first training_size records\n",
    "    y_train = y[:train_size]\n",
    "    X_test = X[train_size:] # the last test_size records\n",
    "    y_test = y[train_size:]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = split_data(X, y, 0.8) # use 80% of the data as training data\n",
    "\n",
    "# check the size of the splitted dataset\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RL1N8mKUWYnx"
   },
   "source": [
    "We’ll not touch the test data except for reporting the errors of our learned models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q2yKNR49Wkn8"
   },
   "source": [
    "## Understanding What We’re Predicting\n",
    "\n",
    "Before we get to training a linear model on the data and using it to make predictions, let’s look\n",
    "at the spread of y values on the training set. The values are integers between 3 and 9 indicating\n",
    "the quality of the wine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-PMpsZNSWthB"
   },
   "source": [
    "### **Task 1**\n",
    "Make a bar chart showing the distribution of y values appearing in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1409,
     "status": "ok",
     "timestamp": 1596436129240,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "4L_JDK3dWrsR",
    "outputId": "71b22bf6-77ce-4bd6-d5b1-61f633923144"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title\n",
    "# Task 1: \n",
    "# the function takes the training dataset as the input, and make the bar chart\n",
    "def plot_bar_chart_score(X_train, y_train):\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    plt.title(\"\") # add title\n",
    "    plt.xlabel(\"Quality\") # add x axis caption\n",
    "    plt.ylabel(\"Frequency\") # add y axis caption\n",
    "    values, count = np.unique(y_train, return_counts = True)\n",
    "    plt.bar(values, count) # draw the plot\n",
    "    plt.show() # show the plot\n",
    "\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "plot_bar_chart_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GxjlElni2FcH"
   },
   "source": [
    "### **Task 2** \n",
    "Implement the trivial predictor, which uses the average value of y on the training set as the prediction for ever datapoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1402,
     "status": "ok",
     "timestamp": 1596436129240,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "-V3xFYexX1lt",
    "outputId": "5e57738e-87d5-408c-f1bf-9df66a175f35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of y on the training label values is 5.878764675855028\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "# Task 2: implement the simplest predictor\n",
    "# The function computes the average value of y on the training label values\n",
    "def compute_average(y_train):\n",
    "    # The code below is just for compilation. \n",
    "    # You need to delete it and write your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    # Hint: return the mean of y\n",
    "    return np.mean(y_train)\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "y_train_avg = compute_average(y_train)\n",
    "print(\"Average of y on the training label values is {}\".format(y_train_avg))\n",
    "\n",
    "# The simplest predictor returns the average value.\n",
    "def simplest_predictor(X_test, y_train_avg):\n",
    "  return y_train_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x531Q_SxXV14"
   },
   "source": [
    "### **Task 3**\n",
    "Report the mean squared error, i.e., the average of the squared residuals, using this simplest of predictors on the training and test data. We should hope that our models beat at lease this baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1394,
     "status": "ok",
     "timestamp": 1596436129240,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "mV8l6Ci9YlgL",
    "outputId": "f57858dc-d0fc-40fe-dbf7-c652d2f8fddb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplest Predictor\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "MSE (Training) = 0.7768\n",
      "MSE (Testing)  = 0.8139\n"
     ]
    }
   ],
   "source": [
    "# We will evaluate our simplest predictor here. \n",
    "# Implement a function that can report the mean squared error \n",
    "# of a predictor on the given test data\n",
    "# Input: test dataset and predictor\n",
    "# Output: mean squared error of the predictor on the given test data\n",
    "def test_data(X_test, y_test, predictor: callable=None):\n",
    "    # Applies the predictor to each row to compute the predicted values\n",
    "    y_predicted = np.apply_along_axis(predictor, 1, X_test)\n",
    "       \n",
    "    # TODO: compute the mean squared error of y_predicted\n",
    "    # The code below is just for compilation. \n",
    "    # You need to delete it and write your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    mse = np.mean(np.square(np.subtract(y_test,y_predicted)))    \n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "    \n",
    "    return mse\n",
    "\n",
    "# use the above function test_data to evaluate the simplest predictor\n",
    "# we use the lambda function here to pass the function simplest_predictor to the evaluator.\n",
    "mse_simplest_predictor_train = test_data(X_train, y_train, lambda x: simplest_predictor(x, y_train_avg))\n",
    "mse_simplest_predictor_test = test_data(X_test, y_test, lambda x: simplest_predictor(x, y_train_avg))\n",
    "\n",
    "# Report the result\n",
    "print('Simplest Predictor')\n",
    "print('--------------------------------------------------------------------------------\\n')\n",
    "print('MSE (Training) = %.4f' % mse_simplest_predictor_train)\n",
    "print('MSE (Testing)  = %.4f' % mse_simplest_predictor_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "geiyM1Nea0az"
   },
   "source": [
    "## Linear Model Using Least Squares\n",
    "\n",
    "Let us first fit a linear regression model and then calculate the training and test error. We’ll\n",
    "actually use the closed form solution of the least squares estimate for the linear model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cRPPA6HMbNOr"
   },
   "source": [
    "### **Task 4**\n",
    "Is it strictly necessary to standardize the data for the linear model using the least squares method? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9he5QMmfqL3_"
   },
   "source": [
    "For the linear model we do NOT need to standardize the data when we are using least squares. The reason for that is our $X$-Matrix (or more precisely the $X^TX$-Matrix), which is invertible. This means we are using an invariant method. As soon as we are using non-invariant methods like Ridge or Lasso, we need to strictly standardize the data. The same goes for the case when you are using Basis Expansion (e.g. because of cross-term features like ($X$1x$X$2) and their respective covariance).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WSEwFGp_bqAI"
   },
   "source": [
    "### **Task 5**\n",
    "Standardize the data, i.e., make the data for every feature have mean 0 and variance 1. \n",
    "\n",
    "We do the standardization using the training data, and we need to remember the means and\n",
    "the standard deviations so that they can be applied to the test data as well. Apply the\n",
    "standardization so that every feature in the training data has mean 0 and variance 1. Apply\n",
    "the same transformation to the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1387,
     "status": "ok",
     "timestamp": 1596436129241,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "trjwkcgybhDH",
    "outputId": "d87a4635-354f-47e2-947a-e843f027e4cb",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_std: (3918, 11)\n",
      "Mean: [6.85427514e+00 2.78390761e-01 3.34892802e-01 6.42623788e+00\n",
      " 4.58213374e-02 3.53263144e+01 1.38513272e+02 9.94040729e-01\n",
      " 3.18647524e+00 4.89055641e-01 1.05115799e+01]\n",
      "Standard deviation: [8.39100902e-01 9.95630176e-02 1.24249975e-01 5.06377532e+00\n",
      " 2.16660282e-02 1.71004677e+01 4.23956179e+01 2.97972269e-03\n",
      " 1.49949475e-01 1.12992053e-01 1.22536544e+00]\n"
     ]
    }
   ],
   "source": [
    "# Input: training data\n",
    "# Output: standardize training data, standard deviations and means\n",
    "def standardize_data(X):\n",
    "    # TODO: compute mean, standard deviations and the standardized data\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    \n",
    "    ####This is the theory\n",
    "    X_std = (X-mean)/std\n",
    "    \n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "    \n",
    "    return X_std, mean, std\n",
    "\n",
    "X_train_std, X_train_mean, X_train_std_div = standardize_data(X_train)\n",
    "print(\"X_train_std:\", X_train_std.shape)\n",
    "print(\"Mean:\", X_train_mean)\n",
    "print(\"Standard deviation:\", X_train_std_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1380,
     "status": "ok",
     "timestamp": 1596436129242,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "RjzbA5JpM759",
    "outputId": "ff594788-2fdd-419c-98fa-beac6a53cfc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(980, 11)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Standardize the test data using the mean and standrad deviation you computed for the training data\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "X_test_std = (X_test-X_train_mean)/X_train_std_div\n",
    "print(X_test_std.shape)\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vT4_Sl42bxmD"
   },
   "source": [
    "### **Task 6**\n",
    "Implement the linear model predictor, and report the mean squared error using the linear model on the training and test data.\n",
    "\n",
    "We will do this in several steps. We need to implement the function for computing the parameters based on the training dataset. Note we need to add the bias column to the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1374,
     "status": "ok",
     "timestamp": 1596436129242,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "A4JtLr6pdJV7",
    "outputId": "dfd57312-284f-4ce9-820b-4fdbdfbec8c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: (12,)\n"
     ]
    }
   ],
   "source": [
    "# the function adds a column of ones to the front of the input matrix\n",
    "def expand_with_ones(X):\n",
    "    # TODO: adds a column of ones to the front of the input matrix\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    ones = np.ones((X.shape[0],1))\n",
    "    X_out = np.c_[ones,X]\n",
    "    return X_out\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "# The function computes the parameters\n",
    "def least_squares_compute_parameters(X_input, y):\n",
    "    # add the bias column to the dataset\n",
    "    X = expand_with_ones(X_input)\n",
    "    # TODO: compute the parameters based on the expanded X and y\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    T = np.linalg.inv((X.T.dot(X))).dot(X.T.dot(y))\n",
    "    return T\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "# train the linear model parameters\n",
    "w = least_squares_compute_parameters(X_train_std, y_train) \n",
    "print(\"w:\", w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lasj_1PpeZib"
   },
   "source": [
    "We then implement the linear model predictor given the dataset and the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lb-hNagxc3Wj"
   },
   "outputs": [],
   "source": [
    "# Implement the linear model predictor\n",
    "# Input: test data and parameters\n",
    "# Output: predicted values\n",
    "def linear_model_predictor(X, w):\n",
    "    # TODO: compute the predicted values based on the test dataset and the parameters\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    return w.T.dot(X)\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cFOYpwbufz7J"
   },
   "source": [
    "We can now evaluate our linear model predictor on the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1363,
     "status": "ok",
     "timestamp": 1596436129243,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "LuHHmn2RB55j",
    "outputId": "b6cb4556-2618-419a-a082-214f2e6ecb5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error is 0.5607292042283472\n"
     ]
    }
   ],
   "source": [
    "# use the function test_data to evaluate the linear model predictor\n",
    "mse_linear_model_predictor = test_data(expand_with_ones(X_test_std), y_test, lambda x: linear_model_predictor(x, w))\n",
    "print(\"Mean squared error is {}\".format(mse_linear_model_predictor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zqj4HKAihF7Q"
   },
   "source": [
    "## Learning Curves\n",
    "\n",
    "Let us see if the linear model is overfitting or underfitting. Since the dataset is somewhat large and there are only 11 features, our guess should be that it may either be underfitting or be about right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XDLCsjzWhMCp"
   },
   "source": [
    "Starting with 20 datapoints, we’ll use training datasets of increasing size, in increments of 20 up to about 600 datapoints. For each case train the linear model only using the first n elements of\n",
    "the training data. Calculate the training error (on the data used) and the test error (on the full test set). Plot the training error and test error as a function of the size of the dataset used for\n",
    "training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MNf11kurCgKF"
   },
   "source": [
    "### **Task 7** \n",
    "Implement a function that evaluates the linear model over the training dataset with the input size.\n",
    "The function takes a dataset and the split coefficient as inputs, and\n",
    "1. splits the data to training and test datasets,\n",
    "2. standardizes the data,\n",
    "3. trains the linear model, and\n",
    "4. reports the mse of the linear model predictor on both training and test datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1355,
     "status": "ok",
     "timestamp": 1596436129244,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "UcGRQBrEb106",
    "outputId": "179c5ec0-ee87-4c4b-a02b-d97d55862e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE using Linear Models\n",
      "-----------------------\n",
      "\n",
      "MSE (Training) = 0.5640\n",
      "MSE (Testing)  = 0.5607\n"
     ]
    }
   ],
   "source": [
    "# Input: dataset and split coefficient\n",
    "# Output: mse of the linear model predictor on both the training and test datasets\n",
    "def train_and_test(X, y, split_coeff):\n",
    "    # TODO: implement the function \n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    # Hints: use the functions you have implemented\n",
    "    \n",
    "    # split data into training and testing sets\n",
    "    X_train, y_train, X_test, y_test = split_data(X,y,split_coeff) \n",
    "    \n",
    "    # standardise the X_train set and obtain mean&std to standardise X_test\n",
    "    X_train_std, X_train_mean, X_train_std_div = standardize_data(X_train)\n",
    "    X_test_std = (X_test-X_train_mean)/X_train_std_div\n",
    "    \n",
    "    # calculate weights with training data\n",
    "    w = least_squares_compute_parameters(X_train_std, y_train)\n",
    "    \n",
    "    # obtain mse train/test\n",
    "    mse_train = test_data(expand_with_ones(X_train_std), y_train, lambda x: linear_model_predictor(x, w))\n",
    "    mse_test = test_data(expand_with_ones(X_test_std), y_test, lambda x: linear_model_predictor(x, w))\n",
    "    return mse_train, mse_test\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "mse_train, mse_test = train_and_test(X, y, 0.8)\n",
    "print('MSE using Linear Models')\n",
    "print('-----------------------\\n')\n",
    "print('MSE (Training) = %.4f' % mse_train)\n",
    "print('MSE (Testing)  = %.4f' % mse_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NTJw_BrzhRwi"
   },
   "source": [
    "### **Task 8**\n",
    "Report the learning curves plot. Also, explain whether you think the model is underfitting or not and how much data you need before getting the optimal test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1747,
     "status": "ok",
     "timestamp": 1596436129644,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "jDsdh4T3hcIU",
    "outputId": "621c4890-1c55-4e9b-f28f-33d60907d8b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowest test error at: 320\n",
      "lowest absolute difference at: 160\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mse_train_v = []\n",
    "mse_test_v = []\n",
    "\n",
    "TRAINING_SIZE_MAX = 601\n",
    "TRAINING_SIZE_MIN = 20\n",
    "\n",
    "\n",
    "# compute the errors over datasets with different sizes\n",
    "for train_size in range(TRAINING_SIZE_MIN, TRAINING_SIZE_MAX, 20):\n",
    "    # TODO: compute the training error and test error on datasets with size train_size\n",
    "    # and add them to mse_train_v and mse_test_v, respectively\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    mse_train, mse_test = train_and_test(X[:train_size],y[:train_size],0.8)\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "    mse_train_v.append(mse_train)\n",
    "    mse_test_v.append(mse_test)  \n",
    "    \n",
    "    \n",
    "# size of dataset where testerror is the lowest    \n",
    "min_index = np.argmin(mse_test_v)\n",
    "training_sizes = np.arange(TRAINING_SIZE_MIN, TRAINING_SIZE_MAX, 20)\n",
    "print(\"lowest test error at:\",training_sizes[min_index])\n",
    "\n",
    "# size of dataset where absolute difference between mse_test and mse_train is lowest\n",
    "min_index2= np.argmin([abs(mse_train_v[i] -mse_test_v[i]) for i in range(len(mse_train_v))])\n",
    "print(\"lowest absolute difference at:\",(training_sizes[min_index2]))\n",
    "\n",
    "# The below code outputs the plot of mse from different training sizes\n",
    "plt.figure(2)\n",
    "plt.plot(np.arange(TRAINING_SIZE_MIN, TRAINING_SIZE_MAX, 20), mse_train_v, 'r--', label=\"Training Error\")\n",
    "plt.plot(np.arange(TRAINING_SIZE_MIN, TRAINING_SIZE_MAX, 20), mse_test_v, 'b-', label=\"Test Error\")\n",
    "plt.xlabel('Dataset Size')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9A9VqDTzOdfd"
   },
   "source": [
    "Out model overfits until we use a dataset size of around 50, then the overfitting stops. It seems to have a rather high bias (error is not close to 0) since a small amount of data leads to the training/test-error to plateau, therefore we might be underfitting. Sadly, adding more data does not help us in this case. We also checked where the lowest test error is, and where the absolute difference of test and train mse is the lowest, but after train and test mse meet, there is not much going on anymore. Perhaps a more complex model could fit/describe the true nature of the data better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djpsaTu_kK3T"
   },
   "source": [
    "## Polynomial Basis Expansion with Ridge and Lasso\n",
    "\n",
    "For this part use the following from the scikit-learn package. Read the documentation available here: http://scikit-learn.org/stable/modules/classes.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pnw2FEvqkdV_"
   },
   "source": [
    "You will need the use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9TM0nkNbkhfM"
   },
   "outputs": [],
   "source": [
    "# You will need the following libs. \n",
    "# Fell free to import other libs. \n",
    "\n",
    "# import the preprocessing libs for standarization and basis expansion\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures \n",
    "\n",
    "# Ridge and Lasso linear model\n",
    "from sklearn.linear_model import Ridge, Lasso "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9fAfOfXCksT9"
   },
   "source": [
    "Try 5 powers of 10 for lambda from 10^-2 to 10^2 and use degree 2 basis expansion. Fit ridge and lasso using degree 2 polynomial expansion with these values of lambda. You should pick the optimal values for lambda using a validation set. Set the last 20% of the training set for the purpose of validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCwBPuOXlRF7"
   },
   "source": [
    "### **Task 9**\n",
    "Let's implement the function for expanding the basis of the dataset. \n",
    "\n",
    "Hints: use `PolynomialFeatures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "50azFolql1qA"
   },
   "outputs": [],
   "source": [
    "def expand_basis(X, degree):\n",
    "    # TODO: expand the basis of X for the degree\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    # Hints: use the function PolynomialFeatures\n",
    "    \n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X = poly.fit_transform(X)\n",
    "    return X\n",
    "    \n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6jwkPevimQri"
   },
   "source": [
    "### **Task 10**\n",
    "Prepare the training, test and validation data using the expanded dataset. Expand and standardize the the data. \n",
    "\n",
    "Hints: you can use `StandardScaler` and `std_scaler` to standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dQCq4G9YmW7w"
   },
   "outputs": [],
   "source": [
    "# TODO: the training, test and validation data using the expanded dataset.\n",
    "# The code below is just for compilation. \n",
    "# You need to replace it by your own code.\n",
    "def prepare_data(X, y, degree):\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    # Hints: follow the steps    \n",
    "    # You need to parpare four datasets:\n",
    "    # 1. training data -- X_train, y_train\n",
    "    # 2. test data -- X_test, y_test\n",
    "    # 3. validation data -- X_train_v, y_train_v\n",
    "    # 4. training data (cross validation) -- X_train_n, y_train_n\n",
    "    \n",
    "    # You need expand the basis of the data, and do standardization\n",
    "    \n",
    "    # 1. Split the data into test & train\n",
    "    coeff = 0.8\n",
    "    X_train, y_train, X_test, y_test = split_data(X, y, coeff)\n",
    "    \n",
    "    # 2. Standardise training & test\n",
    "    scaler = StandardScaler()\n",
    "    scaled_X_train = scaler.fit_transform(X_train)\n",
    "    scaled_X_test = scaler.fit_transform(X_test)\n",
    "    \n",
    "    # 3. Expand basis of training & test\n",
    "    expanded_scaled_X_train = expand_basis(scaled_X_train, degree)\n",
    "    expanded_scaled_X_test = expand_basis(scaled_X_test, degree)\n",
    "    \n",
    "    # 4. Split training into training & validation\n",
    "    X_train_n, y_train_n, X_train_v, y_train_v = split_data(X_train, y_train, coeff)\n",
    "    \n",
    "    # 5. Standardise training & validation\n",
    "    scaled_X_train_n = scaler.fit_transform(X_train_n)\n",
    "    scaled_X_train_v = scaler.fit_transform(X_train_v)\n",
    "    \n",
    "    # 6. Expand basis of training & validation\n",
    "    expanded_scaled_X_train_n = expand_basis(scaled_X_train_n, degree)\n",
    "    expanded_scaled_X_train_v = expand_basis(scaled_X_train_v, degree)\n",
    "    \n",
    "    # Rename stuff\n",
    "    X_train = expanded_scaled_X_train\n",
    "    X_train_n = expanded_scaled_X_train_n\n",
    "    X_train_v = expanded_scaled_X_train_v\n",
    "    X_test = expanded_scaled_X_test\n",
    "\n",
    "    return X_train, y_train, X_train_n, y_train_n, X_train_v, y_train_v, X_test, y_test\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "X_train, y_train, X_train_n, y_train_n, X_train_v, y_train_v, X_test, y_test = prepare_data(X, y, 2) # here we expand the dataset with degree 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i3BxxtM3nghU"
   },
   "source": [
    "### **Task 11**\n",
    "We have prepared the training data and the validation data. We can now choose the hyper parameter lambda for Ridge and Lasso using the validation data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3266,
     "status": "ok",
     "timestamp": 1596436131187,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "SvXcAGW1oHq1",
    "outputId": "25a38d1f-013f-4b0a-9cbb-3f08b68c0371"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramon\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 440.6694341653254, tolerance: 0.24240462667517543\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\ramon\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 28.158705638910988, tolerance: 0.24240462667517543\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge lambda: 10\n",
      "Lasso lambda: 0.001\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The function takes the training and validation data as inputs, and \n",
    "# returns the lambda value that has the minimal mse\n",
    "# We use is_ridge to indicate the model we consider. \n",
    "# is_ridge = True indicates Ridge while is_ridge = False indicates Lasso\n",
    "def choose_hyper_param(X_train_n, y_train_n, X_train_v, y_train_v, is_ridge: bool):\n",
    "    mse_arr = []\n",
    "    lam_arr = []\n",
    "\n",
    "    # Try lambda values from 10^-2 to 10^2. \n",
    "    # Record the mse and the lambda values in mse_arr and lam_arr\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    for pow_lam in range(-4, 3):\n",
    "        lam = 10 ** pow_lam\n",
    "        if is_ridge is True:\n",
    "            regressor = Ridge(alpha = lam)\n",
    "        else:\n",
    "            regressor = Lasso(alpha = lam) #could add \"max_iter=10000000\", but is computationally way more expensive\n",
    "        regressor.fit(X_train_n, y_train_n)\n",
    "        y_predictions = regressor.predict(X_train_v)\n",
    "        mse = np.mean(np.square(np.subtract(y_train_v, y_predictions)))\n",
    "        mse_arr.append(mse) # add the mse when using the hyperparameter lam\n",
    "        lam_arr.append(lam)\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "\n",
    "    # get the index of the lambda value that has the minimal use\n",
    "    lambda_idx_min = np.argmin(np.array(mse_arr))\n",
    "    # print(lam_arr[lambda_idx_min])\n",
    "\n",
    "    # plot of the lambda values and their mse\n",
    "    plt.figure()\n",
    "    plt.semilogx(lam_arr, mse_arr)\n",
    "\n",
    "    # return the best lambda value\n",
    "    return lam_arr[lambda_idx_min]\n",
    "\n",
    "# call the function to choose the lambda for Ridge and Lasso\n",
    "lam_ridge = choose_hyper_param(X_train_n, y_train_n, X_train_v, y_train_v, True)\n",
    "lam_lasso = choose_hyper_param(X_train_n, y_train_n, X_train_v, y_train_v, False)\n",
    "\n",
    "print(\"Ridge lambda:\", lam_ridge)\n",
    "print(\"Lasso lambda:\", lam_lasso)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FAuX0uU5k9qD"
   },
   "source": [
    "### **Task 12**:\n",
    "Once you’ve obtained the optimal values for lambda for Ridge and Lasso, train these models using these hyperparameters on the full training data. Then report\n",
    "the training and test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3259,
     "status": "ok",
     "timestamp": 1596436131187,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "VmwHESkg77zK",
    "outputId": "9bb9c1cf-1649-40e6-9162-2244525d9446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Ridge Regression with using degree 2 polynomial expansion and lambda = 10.0000\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "MSE (Training) = 0.4955\n",
      "MSE (Testing)  = 0.5133\n",
      "\n",
      "\n",
      "For Lasso with using degree 2 polynomial expansion and lambda = 0.0010\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "MSE (Training) = 0.4962\n",
      "MSE (Testing)  = 0.5115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramon\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6056512715367717, tolerance: 0.304341322103114\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# TODO: train the Ridge and Lasso models using their best parameters, and\n",
    "#       report their mse\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "# Hints: train these models on the full training data\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def mse_calc(X_train, X_test, y_train, y_test, lam, is_ridge: bool):\n",
    "    if is_ridge:\n",
    "        regressor = Ridge(alpha=lam)\n",
    "    else:\n",
    "        regressor = Lasso(alpha=lam)\n",
    "    regressor.fit(X_train,y_train)\n",
    "    y_predicted_train = regressor.predict(X_train) \n",
    "    y_predicted_test = regressor.predict(X_test)\n",
    "    mse_train = mean_squared_error(y_predicted_train, y_train)\n",
    "    mse_test = mean_squared_error(y_predicted_test, y_test)\n",
    "    return mse_train, mse_test\n",
    "\n",
    "mse_ridge_train, mse_ridge_test = mse_calc(X_train, X_test, y_train, y_test, lam_ridge, True)\n",
    "mse_lasso_train,mse_lasso_test = mse_calc(X_train, X_test, y_train, y_test, lam_lasso, False)\n",
    "\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################\n",
    "\n",
    "# Report the result\n",
    "print('For Ridge Regression with using degree %d polynomial expansion and lambda = %.4f' % (2, lam_ridge))\n",
    "print('--------------------------------------------------------------------------------\\n')\n",
    "print('MSE (Training) = %.4f' % mse_ridge_train)\n",
    "print('MSE (Testing)  = %.4f' % mse_ridge_test)\n",
    "\n",
    "print('\\n\\nFor Lasso with using degree %d polynomial expansion and lambda = %.4f' % (2, lam_lasso))\n",
    "print('---------------------------------------------------------------------\\n')\n",
    "print('MSE (Training) = %.4f' % mse_lasso_train)\n",
    "print('MSE (Testing)  = %.4f' % mse_lasso_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Os9tKKLd8gMU"
   },
   "source": [
    "## Larger Degrees\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfqRAlv1PBXi"
   },
   "source": [
    "### **Task 13**\n",
    "Try using higher degree basis expansion. You may want to use k-fold cross validation to determine\n",
    "the values of hyperparameters rather than just keeping a validation set. \n",
    "\n",
    "Hints: Use `KFold` to do this automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kpwY7UtQ8l-0",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.5191956033320867\n",
      "3 0.5177749129978928\n"
     ]
    }
   ],
   "source": [
    "# KFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# TODO: Try using higher degree basis expansion. Find the degree that gives the minimal mse. \n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "\n",
    "# Hints: use KFold\n",
    "def prepare_data_2(X1, X2, degree):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X1)\n",
    "    X1 = expand_basis(scaler.transform(X1),degree)\n",
    "    X2 = expand_basis(scaler.transform(X2),degree)\n",
    "    return X1, X2\n",
    "\n",
    "\n",
    "def choose_hyper_params(X_train, y_train, fold, is_ridge: bool):\n",
    "    mse_arr = []\n",
    "    deg_lam_arr = []\n",
    "    kf = KFold(n_splits=10)\n",
    "    \n",
    "    for degree in range(1,5):\n",
    "        for pow_lam in range(-4, 3):\n",
    "            \n",
    "            lam = 10 ** pow_lam\n",
    "            deg_lam_arr.append((degree,lam))\n",
    "            mse_sum = 0.0\n",
    "            \n",
    "            for train_index, val_index in kf.split(X_train):\n",
    "\n",
    "                # Prepare the data with standardisation and basis expansion. \n",
    "                X_train_n, X_train_v = X[train_index], X[val_index]\n",
    "                y_train_n, y_train_v = y[train_index], y[val_index]\n",
    "                X_train_n, X_train_v = prepare_data_2(X_train_n, X_train_v, degree)\n",
    "                \n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "\n",
    "    # get the index of the lambda value that has the minimal use\n",
    "    degree_idx_min_ridge = np.argmin(np.array(mse_ridge_arr))\n",
    "    degree_idx_min_lasso = np.argmin(np.array(mse_lasso_arr))\n",
    "    # print(lam_arr[lambda_idx_min])\n",
    "    # return the best lambda value\n",
    "print(deg_arr[degree_idx_min_ridge],np.min(mse_ridge_arr))\n",
    "print(deg_arr[degree_idx_min_lasso],np.min(mse_lasso_arr))\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP96ktvsOI4PiuW52tcNLjx",
   "collapsed_sections": [],
   "name": "Practical1_starter.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
