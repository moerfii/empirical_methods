{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 2 : Generative and Discriminative Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical, we will compare the Naïve Bayes Classifier (NBC) and Logistic Regression on six\n",
    "datasets. As part of the practical you should briefly read the following paper:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On Discriminative vs. Generative classifiers: A comparison of logistic regression\n",
    "and naive Bayes**  \n",
    "*Andrew Y. Ng and Michael I. Jordan*  \n",
    "Advances in Neural Information Processing Systems (NIPS) 2001.\n",
    "\n",
    "The paper is available on OLAT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should read the Introduction and the Experiments sections. The goal of this practical is\n",
    "to qualitatively reproduce some of the experimental results in this paper. You are strongly\n",
    "encouraged to read the rest of the paper, which is rather short and straightforward to read,\n",
    "though some of you may want to skip the formal proofs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should implement a Naïve Bayes Classifier directly in python. To keep your code tidy,\n",
    "we recommend implementing it as a class. Make sure that your classifier can handle binary, continuous and categorical features, and an arbitrary number of class labels. Suppose the data has 3\n",
    "different features, the first being binary, the second being continuous and the third being categorical, and that there are\n",
    "4 classes. Write an implementation that you can initialise as follows:\n",
    "\n",
    "    nbc = NBC(feature_types=['b', 'r', 'c'], num_classes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along the lines of classifiers provided in sklearn, you want to implement two more functions,\n",
    "**fit** and **predict**. \n",
    "Recall the joint distribution of a generative model: $p(\\mathbf{x}, y \\mid \\theta, \\pi) = p(y | \\pi) \\cdot p(\\mathbf{x} \\mid y, \\theta)$.\n",
    "The fit function is expected to estimate all the parameters ($\\theta$ and $\\pi$) of the NBC. The predict function is expected to compute the probabilities that the new input belongs to all classes and\n",
    "then return the class that has the largest probability.\n",
    "\n",
    "    nbc.fit(X_train, y_train)\n",
    "    ypredicted = nbc.predict(X_test)\n",
    "    test_accuracy = np.mean(ypredicted == ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10., 10.)\n",
    "\n",
    "import pickle as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal, multinomial\n",
    "from scipy.stats import bernoulli\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing NBC, we suggest you first implement the three types of the distributions of the parameters of NBC. Your implementation should have two functions: **estimate** and **get_probability**. The estimate function takes some data as input and computes the maximum likelihood estimators (MLE) for the parameters $\\theta$ of the distribution $p(x | \\theta)$. The get_probability function takes a new input value $x_{new}$ and returns $p(x_{new} | \\theta)$. For example, in the case of continuous features, we can use the Gaussian distribution. The estimate function will find the parameters $\\mu$ and $\\sigma$ for the Gaussian distribution with respect to the input data, and the function get_probability will return $\\mathcal{N}(x_{new} \\mid \\mu, \\sigma)$. \n",
    "\n",
    "![alt text](pics/mle_4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can import statistic libraries for the implementation of the distributions. We recommend using the statistical functions provided by `scipy.stats`. Read the documentation here: https://docs.scipy.org/doc/scipy/reference/stats.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution for continuous features\n",
    "class ContFeatureParam:\n",
    "    def estimate(self, X):\n",
    "        # TODO: Estimate the parameters for the Gaussian distribution \n",
    "        # so that it best describes the input data X\n",
    "        # The code below is just for compilation. \n",
    "        # You need to replace it by your own code.\n",
    "        ###################################################\n",
    "        ##### YOUR CODE STARTS HERE #######################\n",
    "        ###################################################\n",
    "\n",
    "        self.mean = np.mean(X)\n",
    "        self.variance = np.var(X)\n",
    "        if self.variance.size == 1:\n",
    "            if self.variance == 0:\n",
    "                self.variance = 10**-6\n",
    "        else:\n",
    "            for i,val in enumerate(self.variance):\n",
    "                if self.variance[i] == 0 or self.variance[i] == np.inf:\n",
    "                    self.variance[i] = 10**-6\n",
    "        return self.mean, self.variance\n",
    "        # self.mean, self.var = norm.stats(moments=\"mv\")\n",
    "        \n",
    "        ###################################################\n",
    "        ##### YOUR CODE ENDS HERE #########################\n",
    "        ###################################################\n",
    "\n",
    "    def get_probability(self, val, j, dfCont):\n",
    "        # TODO: returns the density value of the input value val\n",
    "        # Note the input value val could be a vector rather than a single value\n",
    "        # The code below is just for compilation. \n",
    "        # You need to replace it by your own code.\n",
    "        ###################################################\n",
    "        ##### YOUR CODE STARTS HERE #######################\n",
    "        ###################################################       \n",
    "        \n",
    "        #Directly\n",
    "        return multivariate_normal.logpdf(val, mean=dfCont[j].iloc[0], cov=dfCont[j].iloc[1])\n",
    "        \n",
    "        # by hand implementation\n",
    "        #pi = np.pi\n",
    "        #return np.sqrt(1/(2*pi*self.variance))*np.exp(-(1/(2*self.variance)*(val-self.mean)**2))\n",
    "    \n",
    "    \n",
    "        ###################################################\n",
    "        ##### YOUR CODE ENDS HERE #########################\n",
    "        ###################################################\n",
    "\n",
    "# Distribution for binary features\n",
    "class BinFeatureParam:\n",
    "    def estimate(self, X):\n",
    "        # TODO: Estimate the parameters for the Bernoulli distribution \n",
    "        # so that it best describes the input data X\n",
    "        # The code below is just for compilation. \n",
    "        # You need to replace it by your own code.\n",
    "        ###################################################\n",
    "        ##### YOUR CODE STARTS HERE #######################\n",
    "        ###################################################\n",
    "        alpha = 1\n",
    "        self.ones = np.sum(X,axis=0)\n",
    "        self.prob = (self.ones+alpha)/(len(X)+2*alpha)\n",
    "        return self.prob\n",
    "        \n",
    "        #count = np.count_nonzero(X)\n",
    "        #othercount = X.size - count\n",
    "        ##theta = count / X.size\n",
    "        ##LL = count * np.log(theta) + othercount * np.log(1-theta)\n",
    "        #\n",
    "        #self.theta_prime = count/(othercount+count)\n",
    "        ###################################################\n",
    "        ##### YOUR CODE ENDS HERE #########################\n",
    "        ###################################################\n",
    "\n",
    "    def get_probability(self, val, j, dfprob):\n",
    "        # TODO: returns the density value of the input value val\n",
    "        # The code below is just for compilation. \n",
    "        # You need to replace it by your own code.\n",
    "        ###################################################\n",
    "        ##### YOUR CODE STARTS HERE #######################\n",
    "        ###################################################\n",
    "        \n",
    "        # val = individual rows of dataset\n",
    "        # j = iterator of nr of classes\n",
    "        # dfprob = df with nr of probability of 1's and (counter-) probability of 0's for each class\n",
    "\n",
    "    \n",
    "        prob = np.multiply(pd.DataFrame(val),dfprob[j])\n",
    "        return np.log(np.sum(prob)[0])\n",
    "        '''\n",
    "        k = np.sum(val)\n",
    "        dfprob[j].iloc[0]**k * dfprob[j].iloc[1]**(val.size-k)\n",
    "        '''       \n",
    "        # out probability per column\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        if val == 1:\n",
    "            bin_p = np.divide(self.ones, self.n)\n",
    "            if bin_p == 0:\n",
    "                return 10**-6\n",
    "            if bin_p == 1:\n",
    "                return 1 - 10**-6\n",
    "            else:\n",
    "                return bin_p\n",
    "        if val == 0:\n",
    "            bin_p = np.divide(self.zeros, self.n)\n",
    "            if bin_p == 0:\n",
    "                return 10**-6\n",
    "            if bin_p == 1:\n",
    "                return 1 - 10**-6\n",
    "            else:\n",
    "                return bin_p\n",
    "        else:\n",
    "            print(\"Non-Binary!\")\n",
    "        '''\n",
    "        #return self.theta_prime if val == 1 else 1-self.theta_prime\n",
    "        ###################################################\n",
    "        ##### YOUR CODE ENDS HERE #########################\n",
    "        ###################################################\n",
    "        \n",
    "\n",
    "# Distribution for categorical features\n",
    "class CatFeatureParam:\n",
    "    def estimate(self, X):\n",
    "        # TODO: Estimate the parameters for the Multinoulli distribution \n",
    "        # so that it best describes the input data X\n",
    "        # The code below is just for compilation. \n",
    "        # You need to replace it by your own code.\n",
    "        ###################################################\n",
    "        ##### YOUR CODE STARTS HERE #######################\n",
    "        ###################################################\n",
    "        \n",
    "        \n",
    "        df=pd.DataFrame(X)\n",
    "        \n",
    "        self.countlist = []\n",
    "        for c in df.columns:\n",
    "                    self.countlist.append(df[c].value_counts(normalize=True))\n",
    "        return self.countlist\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################################################\n",
    "        ##### YOUR CODE ENDS HERE #########################\n",
    "        ###################################################\n",
    "\n",
    "    def get_probability(self, val, j, dfCat):\n",
    "        # TODO: returns the density value of the input value val\n",
    "        # The code below is just for compilation. \n",
    "        # You need to replace it by your own code.\n",
    "        ###################################################\n",
    "        ##### YOUR CODE STARTS HERE #######################\n",
    "        ###################################################\n",
    "        \n",
    "        ## val = rows of Xtest\n",
    "        ## j = iterator of class\n",
    "        ## dfCAt = estimators with keys being features rangeing form 3 to 0  and respective probs\n",
    "        \n",
    "        self.probabilities = []\n",
    "        for index,value in enumerate(val):\n",
    "            self.probabilities.append(dfCat[j].iloc[index][val[index]])\n",
    "        self.probabilities = np.product(self.probabilities)\n",
    "        return np.log(self.probabilities)\n",
    "        \n",
    "        '''\n",
    "        if val in self.features:\n",
    "            position = np.where(self.features==val)\n",
    "            mul_p = self.probs[position]\n",
    "            if mul_p == 1:\n",
    "                return 1 - 10**-6\n",
    "            else:\n",
    "                return mul_p\n",
    "            \n",
    "        else:\n",
    "            return 10**-6\n",
    "        '''\n",
    "        #return multinomial.pmf(val, 1, self.probs)\n",
    "        \n",
    "        ###################################################\n",
    "        ##### YOUR CODE ENDS HERE #########################\n",
    "        ###################################################\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.0, 0.6666666666666666)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7fc484dc963c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mest\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtest_cont\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_cont\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_cont\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_probability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-72dd4e38817c>\u001b[0m in \u001b[0;36mget_probability\u001b[1;34m(self, val, j, dfCont)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m#Directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmultivariate_normal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogpdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdfCont\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdfCont\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m# by hand implementation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "#Test Gauss\n",
    "x = np.array([1,2,3,3,2,1])\n",
    "\n",
    "test_cont = ContFeatureParam()\n",
    "est= test_cont.estimate(x)\n",
    "print(test_cont.estimate(x))\n",
    "print(test_cont.get_probability(2,1,est))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Bernoulli\n",
    "x = np.array([1,1,1,1,1,1])\n",
    "test_bin = BinFeatureParam()\n",
    "test_bin.estimate(x)\n",
    "\n",
    "print(test_bin.estimate(x))\n",
    "test_bin.get_probability(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Multi\n",
    "x = np.array([2,2,2])\n",
    "test_mul = CatFeatureParam()\n",
    "np.array\n",
    "print(test_mul.estimate(x))\n",
    "print(test_mul.get_probability(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now implement a class for NBC. We'll keep it simple and try to follow the sklearn models. We'll have an init function, fit function and predict function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints for function fit**: Recall the joint distribution of a generative model: $p(\\mathbf{x}, y \\mid \\theta, \\pi) = p(y | \\pi) \\cdot p(\\mathbf{x} \\mid y, \\theta)$. \n",
    "The fit function will estimate the parameters for NBC based on the training data. \n",
    "Here we give you some hints how to estimate the $\\theta$ in $p(\\mathbf{x} \\mid y, \\theta)$. \n",
    "\n",
    "For each class $c$, we want to estimate the $\\theta_c$ for the distribution $p(\\mathbf{x} \\mid y = c, \\theta_c)$. \n",
    "Since the assumption of NBC that the features are conditionally independent given the class $c$, the class conditional distribution is a product of $D$ distributions, one for each feature: $p(\\mathbf{x} \\mid y = c, \\theta_c) = \\prod_{j}^{D} p(x_j \\mid y = c, \\theta_{jc})$. Hence, we need to estimate the $\\theta_{jc}$ based on the data with class $c$ and feature $j$. \n",
    "\n",
    "![alt text](pics/fit_4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints for function predict**: The predict function should compute the probabilities $p(y = c \\mid \\mathbf{x}_{new}, \\pi, \\theta)$ for the new inputs $\\mathbf{x}_{new}$ on all classes by applying the Bayes rule:\n",
    "\n",
    "$$p(y = c \\mid \\mathbf{x}_{new}, \\pi, \\theta) = \\frac{p(y = c \\mid \\pi_c) \\cdot p(\\mathbf{x}_{new} \\mid y=c, \\theta)}{\\sum^{C}_{c'=1}p(y=c' \\mid \\pi_{c'}) \\cdot p(\\mathbf{x}_{new} \\mid y=c', \\theta_{c'})},$$\n",
    "\n",
    "and then return the class that has the largest probability:\n",
    "\n",
    "$$y_{predict} = \\underset{c}{arg\\,\\max} \\, {p(y = c \\mid \\mathbf{x}_{new}, \\theta_c)}.$$\n",
    "\n",
    "Here we give you some hints on the computation of $p(\\mathbf{x}_{new} \\mid y=c, \\theta_c)$. \n",
    "Due to the conditional independence assumption, we have $p(\\mathbf{x}_{new} \\mid y=c, \\theta_c) = \\prod_{j}^{D} p(x^j_{new} \\mid y = c, \\theta_{jc})$. Since we have got the parameters $\\theta_{jc}$ in the fit phase,  we can use them to compute the probabilities for the new data. \n",
    "\n",
    "![alt text](pics/predict_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBC:\n",
    "    # Inputs:\n",
    "    #   feature_types: the array of the types of the features, e.g., feature_types=['r', 'r', 'r', 'r']\n",
    "    #   num_classes: number of classes of labels\n",
    "    def __init__(self, feature_types=[], num_classes=0):\n",
    "        # The code below is just for compilation. \n",
    "        # You need to replace it by your own code.\n",
    "        ###################################################\n",
    "        ##### YOUR CODE STARTS HERE #######################\n",
    "        ###################################################\n",
    "        self.feature_types = feature_types\n",
    "        self.num_classes = num_classes\n",
    "        ###################################################\n",
    "        ##### YOUR CODE ENDS HERE #########################\n",
    "        ###################################################\n",
    "    # The function uses the input data to estimate all the parameters of the NBC\n",
    "    # You should use the parameters based on the types of the features\n",
    "    def fit(self, X, y):\n",
    "        # The code below is just for compilation. \n",
    "        # You need to replace it by your own code.\n",
    "        ###################################################\n",
    "        ##### YOUR CODE STARTS HERE #######################\n",
    "        ###################################################\n",
    "        \n",
    "        # get list of iterators for nr of classes\n",
    "        self.classes = np.arange(self.num_classes)\n",
    "        \n",
    "        # writing masks to split columns into feature groups\n",
    "        self.ContFeatureList = np.zeros(len(self.feature_types))\n",
    "        self.BinFeatureList = np.zeros(len(self.feature_types))\n",
    "        self.CatFeatureList = np.zeros(len(self.feature_types))\n",
    "        \n",
    "        for i,ftype in enumerate(self.feature_types):\n",
    "            if ftype == \"r\":\n",
    "                self.ContFeatureList[i] = 1\n",
    "                \n",
    "            elif ftype == \"b\":\n",
    "                self.BinFeatureList[i] = 1\n",
    "                \n",
    "            elif ftype == \"c\":\n",
    "                self.CatFeatureList[i] = 1\n",
    "                \n",
    "            else:\n",
    "                print(\"{} is not a valid type \".format(ftype))\n",
    "        \n",
    "\n",
    "        df = pd.DataFrame(X)\n",
    "        df2 = pd.DataFrame(y)\n",
    "        \n",
    "        # Continous Feature Processing (& Mean per Class Calculation for further Processing )\n",
    "        Contdf = df.loc[:,self.ContFeatureList[:] == 1] # apply mask to dataframe\n",
    "        ContEstimates = []\n",
    "        self.MeanPerClass = []\n",
    "        self.ContFeatureObject = ContFeatureParam()\n",
    "        for i in self.classes:\n",
    "            Contdfcopy = Contdf.copy()\n",
    "            Contdfcopy = Contdfcopy.loc[df2[0] == i] # select rows where class = c\n",
    "            ContEstimates.append(self.ContFeatureObject.estimate(Contdfcopy))   \n",
    "            self.MeanPerClass.append(np.divide(np.size(df2.loc[df2[0] == i]),np.size(df2)))\n",
    "        \n",
    "        # Binary Feature Processing\n",
    "        Bindf = df.loc[:,self.BinFeatureList[:] == 1]\n",
    "        BinEstimates = []\n",
    "        self.BinFeatureObject = BinFeatureParam()\n",
    "        for i in self.classes:\n",
    "            Bindfcopy = Bindf.copy()\n",
    "            Bindfcopy = Bindfcopy.loc[df2[0] == i]\n",
    "            BinEstimates.append(self.BinFeatureObject.estimate(Bindfcopy))  \n",
    "        \n",
    "        # Categorical Feature Processing\n",
    "        Catdf = df.loc[:,self.CatFeatureList[:] == 1]\n",
    "        CatEstimates = []\n",
    "        self.CatFeatureObject = CatFeatureParam()\n",
    "        for i in self.classes:\n",
    "            Catdfcopy = Catdf.copy()\n",
    "            Catdfcopy = Catdfcopy.loc[df2[0] == i]\n",
    "            CatEstimates.append(self.CatFeatureObject.estimate(Catdfcopy))  \n",
    "        \n",
    "        \n",
    "        \n",
    "        # place in dataframe for convenience\n",
    "        self.dfCont = []\n",
    "        for c in range(len(ContEstimates)):\n",
    "            n,d = pd.DataFrame(ContEstimates[c]).shape\n",
    "            ContEstimatesdf = pd.DataFrame(ContEstimates[c])+0.00001*np.random.rand(n, d)\n",
    "            self.dfCont.append(ContEstimatesdf.fillna(10**-6))\n",
    "    \n",
    "        self.dfBin = []\n",
    "        for c in range(len(BinEstimates)):\n",
    "            self.dfBin.append(pd.DataFrame(BinEstimates[c]).fillna(10**-6))\n",
    "\n",
    "        \n",
    "        self.dfCat = []\n",
    "        for c in range(len(CatEstimates)):\n",
    "            self.dfCat.append(pd.DataFrame(CatEstimates[c]).fillna(10**-6))\n",
    "\n",
    "        \n",
    "       \n",
    "    \n",
    "        ###################################################\n",
    "        ##### YOUR CODE ENDS HERE #########################\n",
    "        ###################################################\n",
    "                \n",
    "    # The function takes the data X as input, and predicts the class for the data\n",
    "    def predict(self, X):\n",
    "        # The code below is just for compilation. \n",
    "        # You need to replace it by your own code.\n",
    "        ###################################################\n",
    "        ##### YOUR CODE STARTS HERE #######################\n",
    "        ###################################################\n",
    "        # return yhat based on estimators calculated in fit\n",
    "        dftest = pd.DataFrame(X)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        pdf = []\n",
    "        \n",
    "        # Continous \n",
    "        dftestCont = dftest.loc[:,self.ContFeatureList[:] == 1]\n",
    "        if dftestCont.empty:\n",
    "            pass\n",
    "        else:\n",
    "            for i,row in enumerate(dftestCont.values):\n",
    "                pdf_p_class1 = []\n",
    "                for j in self.classes:\n",
    "                    pdf_p_class1.append(self.ContFeatureObject.get_probability(dftestCont.values[i], j, \n",
    "                                                                              self.dfCont))\n",
    "                pdf.append(pdf_p_class1)\n",
    "                \n",
    "        # Binary\n",
    "        dftestBin = dftest.loc[:,self.BinFeatureList[:] == 1]\n",
    "        if dftestBin.empty:\n",
    "            pass\n",
    "        else:\n",
    "            for i,row in enumerate(dftestBin.values):\n",
    "                pdf_p_class2 = []\n",
    "                for j in self.classes:\n",
    "                    pdf_p_class2.append(self.BinFeatureObject.get_probability(dftestBin.values[i], j, \n",
    "                                                                              self.dfBin))\n",
    "                pdf.append(pdf_p_class2)\n",
    "        \n",
    "        \n",
    "        # Categorical\n",
    "        dftestCat = dftest.loc[:,self.CatFeatureList[:] == 1]\n",
    "        if dftestCat.empty:\n",
    "            pass\n",
    "        else:\n",
    "            for i,row in enumerate(dftestCat.values):\n",
    "                pdf_p_class3 = []\n",
    "                for j in self.classes:\n",
    "                    pdf_p_class3.append(self.CatFeatureObject.get_probability(dftestCat.values[i], j, \n",
    "                                                                              self.dfCat))\n",
    "                pdf.append(pdf_p_class3)\n",
    "        \n",
    "        return np.argmax((np.multiply(np.array(pdf),self.MeanPerClass)) ,axis=1)\n",
    "        \n",
    "        ###################################################\n",
    "        ##### YOUR CODE ENDS HERE #########################\n",
    "        ###################################################\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation Issues**\n",
    "- Fell free to add auxiliary functions. \n",
    "- Don't forget to compute $p(y=c | \\pi)$ \n",
    "- Remember to do all the calculations in log space to avoid running into underflow issues. Read more: (Mur) Chapter 3.5.3\n",
    "- Your implementation should be able to handle missing values\n",
    "- As far as possible use matrix operations. So assume that Xtrain, ytrain, Xtest will all\n",
    "be numpy arrays. Try and minimise your use of python loops. (In general, looping over\n",
    "classes or features is OK, but looping over data is probably not a good idea.)\n",
    "- The variance parameter for Gaussian distributions should never be exactly 0, so in\n",
    "case your calculated variance is 0, you may want to set it to a small value such as 1e − 6.\n",
    "Note that this is essential to ensure that your code never encounters division by zero or\n",
    "taking logarithms of 0 errors. Also, you want to ensure that the estimates for the parameter for the Bernoulli or Multinoulli random variables\n",
    "is never exactly 0 or 1. For this reason you should consider using Laplace smoothing (https://en.wikipedia.org/wiki/Additive_smoothing).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the below code to do sanity check of your implementation using the iris dataset. All features of the iris dataset are continuous, so you do not need to implement all types of feature parameters to check your code. \n",
    "\n",
    "You should expect your implementation has an accuracy larger than 90%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n",
      "[0 0 2 2 1 0 0 0 2 1 0 0 2 2 2 0 0 2 1 1 1 2 2 1 2 1 1 2 2 2]\n",
      "[0 0 2 2 1 0 0 0 2 1 0 0 2 1 2 0 0 2 1 1 1 2 2 1 2 1 1 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X, y = iris['data'], iris['target']\n",
    "\n",
    "N, D = X.shape\n",
    "Ntrain = int(0.8 * N)\n",
    "shuffler = np.random.RandomState(seed=4).permutation(N)\n",
    "Xtrain = X[shuffler[:Ntrain]]\n",
    "ytrain = y[shuffler[:Ntrain]]\n",
    "Xtest = X[shuffler[Ntrain:]]\n",
    "ytest = y[shuffler[Ntrain:]]\n",
    "\n",
    "\n",
    "nbc_iris = NBC(feature_types=['r', 'r', 'r', 'r'], num_classes=3)\n",
    "test = nbc_iris.fit(Xtrain, ytrain)\n",
    "\n",
    "yhat = nbc_iris.predict(Xtest)\n",
    "test_accuracy = np.mean(yhat == ytest)\n",
    "print(\"Accuracy:\", test_accuracy)\n",
    "\n",
    "print(yhat)\n",
    "print(ytest)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(Xtrain, ytrain).predict(Xtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 2, 1, 0, 0, 0, 2, 1, 0, 0, 2, 1, 2, 0, 0, 2, 1, 1, 1, 2,\n",
       "       2, 1, 2, 1, 1, 2, 2, 2])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "dftest = pd.DataFrame(test)\n",
    "if dftest.empty:\n",
    "    print(\"empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic regression, you should use the implementation in sklearn. Adding the following\n",
    "line will import the LR model.\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the information provided on the following links to understand some details about how the\n",
    "logistic regression model is implemented in scikit-learn.\n",
    "- http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing NBC and LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "You will compare the classification error of the NBC and LR trained on increasingly\n",
    "larger training datasets. Because the datasets are so small, you should do this multiple times and\n",
    "average the classification error. One run should look as follows:\n",
    "- Shuffle the data, put 20% aside for testing.\n",
    "    \n",
    "    ```N, D = X.shape\n",
    "    Ntrain = int(0.8 * N)\n",
    "    shuffler = np.random.permutation(N)\n",
    "    Xtrain = X[shuffler[:Ntrain]]\n",
    "    ytrain = y[shuffler[:Ntrain]]\n",
    "    Xtest = X[shuffler[Ntrain:]]\n",
    "    ytest = y[shuffler[Ntrain:]]\n",
    "    \n",
    "    ```  \n",
    "\n",
    "\n",
    "- Train the classifiers with increasingly more data. For example, we can train classifiers with 10%, 20%, ..., 100% of the training data. For each case store the classification errors on the test set of the classifiers.\n",
    "\n",
    "You may want to repeat this with at least 200 random permutations (possibly as large as 1000)\n",
    "to average out the test error across the runs. In the end, you will get average test errors as a\n",
    "function of the size of the training data. Plot these curves for NBC and LR on the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs:\n",
    "#   nbc: Naive Bayes Classifier\n",
    "#   lr: Logistic Regression Classifier\n",
    "#   X, y: data\n",
    "#   num_runs: we need repeat num_runs times and store average results\n",
    "#   num_splits: we want to compare the two models on increasingly larger training sets.\n",
    "#               num_splits defines the number of increasing steps. \n",
    "# outputs:\n",
    "#   the arrays of the test errors across the runs of the two classifiers \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import math\n",
    "def compareNBCvsLR(nbc, lr, X, y, num_runs=200, num_splits=10):\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    N, D = X.shape\n",
    "    tst_errs_nbc_final = []\n",
    "    tst_errs_lr_final = []\n",
    "    ## init errors\n",
    "    Ntrain = int(0.8 * N)\n",
    "    shuffler = np.random.RandomState(seed=4).permutation(N)\n",
    "    Xtrain = X[shuffler[:Ntrain]]\n",
    "    ytrain = y[shuffler[:Ntrain]]\n",
    "    Xtest = X[shuffler[Ntrain:]]\n",
    "    ytest = y[shuffler[Ntrain:]]\n",
    "    \n",
    "    for i in range (0,num_runs):\n",
    "        shuffler2 = np.random.permutation(N)\n",
    "        Xtrain = Xtrain[shuffler2]\n",
    "        ytrain = ytrain[shuffler2]\n",
    "        tst_errs_nbc = np.zeros((num_splits))\n",
    "        tst_errs_lr = np.zeros((num_splits))\n",
    "        \n",
    "        for i in range(1,num_splits+1):\n",
    "            N, D = Xtrain.shape\n",
    "            n = N * i/num_splits\n",
    "            Xtrain_reduced = Xtrain[:math.floor(n)]\n",
    "            ytrain_reduced = ytrain[:math.floor(n)]\n",
    "            \n",
    "            ## nbc\n",
    "            nbc.fit(Xtrain_reduced,ytrain_reduced)\n",
    "            yhatnbc = nbc.predict(Xtest)\n",
    "            tst_errs_nbc[i-1] = np.mean(yhatnbc == ytest)\n",
    "\n",
    "            ## lr\n",
    "            lr.fit(Xtrain_reduced,ytrain_reduced)\n",
    "            yhatlr = lr.predict(Xtest)\n",
    "            tst_errs_lr[i-1] = np.mean(yhatlr == ytest)\n",
    "        \n",
    "        \n",
    "        tst_errs_nbc_final.append(tst_errs_nbc)\n",
    "        tst_errs_lr_final.append(tst_errs_lr)\n",
    "    \n",
    "    pd.DataFrame(tst_errs_nbc_final).mean(axis=0)\n",
    "    pd.DataFrame(tst_errs_lr_final).mean(axis=0)\n",
    "    return tst_errs_nbc, tst_errs_lr\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility function below defines the function for plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePlot(nbc_perf, lr_perf, title=None):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    ax.tick_params(axis='both', labelsize=20)\n",
    "\n",
    "    ax.set_xlabel('Percent of training data used', fontsize=20)\n",
    "    ax.set_ylabel('Classification Error', fontsize=20)\n",
    "    if title is not None: ax.set_title(title, fontsize=25)\n",
    "\n",
    "    xaxis_scale = [(i + 1) * 10 for i in range(10)]\n",
    "    plt.plot(xaxis_scale, nbc_perf, label='Naive Bayes')\n",
    "    plt.plot(xaxis_scale, lr_perf, label='Logistic Regression', linestyle='dashed')\n",
    "    \n",
    "    ax.legend(loc='upper right', fontsize=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks: For each dataset,\n",
    "1. prepare the data for the two classifiers\n",
    "2. compare the two classifiers on the dataset and generate the plots\n",
    "3. write a short report of how you prepare the data and your observations of the comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for data preprocessing\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 1: Iris Dataset**\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# TODO: insert your code for experiments\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "\n",
    "# Load data\n",
    "from sklearn.datasets import load_iris\n",
    "iris_obj = load_iris()\n",
    "# the code transform the iris dataset to a dataframe\n",
    "iris = pd.DataFrame(iris_obj.data, columns=iris_obj.feature_names,index=pd.Index([i for i in range(iris_obj.data.shape[0])])).join(pd.DataFrame(iris_obj.target, columns=pd.Index([\"species\"]), index=pd.Index([i for i in range(iris_obj.target.shape[0])])))\n",
    "\n",
    "# Shuffle \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = iris.drop(\"species\", axis=1).to_numpy()\n",
    "y = iris[\"species\"].to_numpy()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=1154)\n",
    "\n",
    "# Normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(Xtrain)\n",
    "Xtrain = scaler.transform(Xtrain)\n",
    "Xtest = scaler.transform(Xtest)\n",
    "\n",
    "# NBC\n",
    "NBC_iris = NBC(feature_types=['r','r','r','r'], num_classes=3)\n",
    "LR_iris = LogisticRegression()\n",
    "results1, results2 = compareNBCvsLR(NBC_iris, LR_iris, Xtrain, ytrain, num_runs=200, num_splits=10)\n",
    "\n",
    "NBC_iris.fit(Xtrain,ytrain)\n",
    "yhat= NBC_iris.predict(Xtest)\n",
    "test_accuracy = np.mean(yhat == ytest)\n",
    "print(\"Accuracy:\", test_accuracy)\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "makePlot(results1, results2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report\n",
    "\n",
    "In order to prepare the data, we did not have to do much, as there were no missing values. We just split the data into test & training and then we scale it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.625     , 0.95833333, 0.95833333, 0.95833333, 0.95833333,\n",
       "        0.95833333, 0.95833333, 0.95833333, 0.95833333, 0.95833333]),\n",
       " array([0.625     , 0.75      , 0.91666667, 0.91666667, 0.91666667,\n",
       "        0.95833333, 0.95833333, 0.95833333, 0.95833333, 0.95833333]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results1, results2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 2: Voting Dataset**\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/congressional+voting+records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5550516502897456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ben\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# TODO: insert your code for experiments\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "\n",
    "# Load Data\n",
    "voting = pd.read_csv('./datasets/datasets/voting.csv')\n",
    "\n",
    "# Drop two columns with most missing values, then drop all rows with missing values\n",
    "voting = voting.drop(\"export-administration-act-south-africa\", axis=1).drop(\"water-project-cost-sharing\", axis=1)\n",
    "voting = voting.dropna()\n",
    "\n",
    "# Ordinal Encoding\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "# # Encoding\n",
    "# voting_encoded = ordinal_encoder.fit_transform(voting) # Rep=1, Dem=0 // Yes=1, No=0\n",
    "\n",
    "# Party encoding\n",
    "voting_party = voting[[\"label\"]]\n",
    "voting_party_encoded = ordinal_encoder.fit_transform(voting_party) # Rep=1, Dem=0\n",
    "\n",
    "# Voting encoding\n",
    "voting_votes = voting.drop(\"label\", axis=1)\n",
    "voting_votes_encoded = ordinal_encoder.fit_transform(voting_votes) # Yes=1, No=0\n",
    "\n",
    "# Shuffle \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = voting_votes_encoded\n",
    "y = voting_party_encoded\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=1154)\n",
    "\n",
    "# NBC\n",
    "NBC_voting = NBC(feature_types=['b','b','b','b','b','b','b','b','b','b','b','b','b','b',], num_classes=2)\n",
    "NBC_voting.fit(Xtrain, ytrain)\n",
    "#print(library)\n",
    "yhat = NBC_voting.predict(Xtest)\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "gnb = BernoulliNB()\n",
    "\n",
    "y_pred = gnb.fit(Xtrain, ytrain).predict(Xtest)\n",
    "y_pred\n",
    "test_accuracy = np.mean(yhat == ytest)\n",
    "print(\"Accuracy:\", test_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Report\n",
    "\n",
    "Lots of rows contain missing data. Looking at the data, we could identify two columns with lots of missing data. We dropped those columns before we dropped the rest of the missing values. With this strategy, we could preserve more than 3/4 of the dataset (compared to 1/2 if one just drops all rows with missing datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npdf = []\\n    \\n# Binary\\ndftestBin = dftest\\nif dftestBin.empty:\\n    pass\\nelse:\\n\\n    for i,row in enumerate(dftestBin.values):\\n        pdf_p_class2 = []\\n        for j in [0,1]:\\n            pdf_p_class2.append(dftestBin.values[i],j,dfBin)\\n        pdf.append(pdf_p_class2)\\n        \\n'"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pdf = []\n",
    "    \n",
    "# Binary\n",
    "dftestBin = dftest\n",
    "if dftestBin.empty:\n",
    "    pass\n",
    "else:\n",
    "\n",
    "    for i,row in enumerate(dftestBin.values):\n",
    "        pdf_p_class2 = []\n",
    "        for j in [0,1]:\n",
    "            pdf_p_class2.append(dftestBin.values[i],j,dfBin)\n",
    "        pdf.append(pdf_p_class2)\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 3: Car Evaluation Dataset**\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/car+evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ben\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.26715727221089913\n"
     ]
    }
   ],
   "source": [
    "# TODO: insert your code for experiments\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "# Load Data\n",
    "car = pd.read_csv('./datasets/datasets/car.csv')\n",
    "\n",
    "# Ordinal Encoding\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder(categories=[[\"vhigh\",\"high\",\"med\",\"low\"],[\"5more\",\"4\",\"3\",\"2\"],[\"more\",\"4\",\"2\"],[\"big\",\"med\",\"small\"],[\"high\",\"med\",\"low\"],[\"vgood\",\"good\",\"acc\",\"unacc\"]])\n",
    "car_attributes = car.drop(\"buying\", axis=1)\n",
    "X = ordinal_encoder.fit_transform(car_attributes)\n",
    "ordinal_encoder = OrdinalEncoder(categories=[[\"vhigh\",\"high\",\"med\",\"low\"]])\n",
    "car_buying = car[[\"buying\"]]\n",
    "y = ordinal_encoder.fit_transform(car_buying)\n",
    "\n",
    "######\n",
    "#Info#\n",
    "######\n",
    "\n",
    "# Byuing: vhigh=3, med=2, low=1, high=0\n",
    "# maint: vhigh=3, med=2, low=1, high=0\n",
    "# doors: 5more=3, 4=2, 3=1, 2=0\n",
    "# persons: more=2, 4=1, 2=0\n",
    "# lug_boot: small=2, med=1, big=0\n",
    "# safety: med=2, low=1, high=0\n",
    "# accept.: vgood=3, unacc=2, good=1, acc=0\n",
    "\n",
    "# Shuffle \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=1154)\n",
    "\n",
    "# NBC\n",
    "NBC_car = NBC(feature_types=['c','c','c','c','c','c'], num_classes=4)\n",
    "\n",
    "test = NBC_car.fit(Xtrain,ytrain)\n",
    "#test[0].where(test[0].keys()==Xtrain[0])\n",
    "\n",
    "\n",
    "'''\n",
    "j = [0,1,2,3]\n",
    "probabilities = []\n",
    "for index,value in enumerate(Xtrain[0]):\n",
    "    print(index,value)\n",
    "    probabilities.append(test[1].iloc[index][Xtrain[0][index]])\n",
    "#np.prod(templist)\n",
    "np.product(probabilities)\n",
    "'''\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "gnb = MultinomialNB()\n",
    "\n",
    "y_pred = gnb.fit(Xtrain, ytrain).predict(Xtest)\n",
    "yhat=NBC_car.predict(Xtest)\n",
    "test_accuracy = np.mean(yhat == ytest)\n",
    "print(\"Accuracy:\", test_accuracy)\n",
    "\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Report\n",
    "\n",
    "Except the one hot encoding, there was not much to do. We added one hot encoding, because some categories are not sorted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 4: Breast Cancer Dataset**\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/breast+cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5918367346938775\n"
     ]
    }
   ],
   "source": [
    "# TODO: insert your code for experiments\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "# Load Data\n",
    "cancer = pd.read_csv('./datasets/datasets/breast-cancer.csv')\n",
    "\n",
    "# Delete rows with value ?\n",
    "node_caps_na = cancer[cancer[\"node-caps\"]==\"?\"].index\n",
    "breast_quad_na = cancer[cancer[\"breast-quad\"]==\"?\"].index\n",
    "cancer.drop(node_caps_na, inplace=True)\n",
    "cancer.drop(breast_quad_na, inplace=True)\n",
    "\n",
    "# Change degree of malignancy to categorical value\n",
    "cancer[\"deg-malig\"] = cancer[\"deg-malig\"].apply(lambda x: str(x))\n",
    "\n",
    "# Ordinal Encoding\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder(categories=[['20-29', '30-39', '40-49', '50-59', '60-69', '70-79'],['premeno','ge40', 'lt40'],['0-4', '5-9', '10-14', '15-19', '20-24', '25-29', '30-34', '35-39','40-44', '45-49', '50-54'],['0-2','3-5','6-8','9-11','12-14','15-17','24-26'],[\"no\",\"yes\"],[\"1\",\"2\",\"3\"],[\"left\",\"right\"],['central', 'left_low', 'left_up', 'right_low', 'right_up'],[\"no\",\"yes\"]])\n",
    "cancer_attributes = cancer.drop(\"Class\", axis=1)\n",
    "cancer_attributes_encoded = ordinal_encoder.fit_transform(cancer_attributes)\n",
    "ordinal_encoder = OrdinalEncoder(categories=[[\"no-recurrence-events\",\"recurrence-events\"]])\n",
    "cancer_recurrence= cancer[[\"Class\"]]\n",
    "cancer_recurrence_encoded = ordinal_encoder.fit_transform(cancer_recurrence)\n",
    "\n",
    "\n",
    "######\n",
    "#Info#\n",
    "######\n",
    "\n",
    "# no cancer recurrence = 0, cancer recurrence = 1\n",
    "# age [0,1,2,3,4,5] = '20-29', '30-39', '40-49', '50-59', '60-69', '70-79'\n",
    "# menopause [0,1,2] = ['ge40', 'lt40', 'premeno'] ## fix order ORDAAAAA\n",
    "# tumor-size [0,1,2,3,4,5,6,7,8,9,10]= '0-4', '10-14', '15-19', '20-24', '25-29', '30-34', '35-39','40-44', '45-49', '5-9', '50-54']\n",
    "# inv-nodes [0,1,2,3,4,5,6] = ['0-2', '12-14', '15-17', '24-26', '3-5', '6-8', '9-11'] ### FIX ORDA\n",
    "# node-caps [0,1] = [no, yes]\n",
    "# deg-malig [0,1,2] = [1,2,3]\n",
    "# breast [0,1] = [left,right]\n",
    "# breast-quad [0,1,2,3,4] = ['central', 'left_low', 'left_up', 'right_low', 'right_up'] \n",
    "# irradiat [0,1] = [no, yes]\n",
    "\n",
    "# Shuffle \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = cancer_attributes_encoded\n",
    "y = cancer_recurrence_encoded\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=1154)\n",
    "\n",
    "# NBC\n",
    "NBC_cancer = NBC(feature_types=['c','c','c','c','b','c','b','c','b'], num_classes=2)\n",
    "NBC_cancer.fit(Xtrain,ytrain)\n",
    "yhat =NBC_cancer.predict(Xtest)\n",
    "test_accuracy = np.mean(yhat == ytest)\n",
    "print(\"Accuracy:\", test_accuracy)\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report\n",
    "\n",
    "First, we deleted all rows which contained an ? value. We then realised that pandas did not read degree of malignancy as a category (which suits much better in our opinion), so we changed that as well. We tried to sort all categorical values as well, so the corresponding number would be sorted. However, we could not make it work, so we sticked to the order that OrdinalEncoder() provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 5: Ionosphere Dataset**\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/ionosphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.586788335647689\n"
     ]
    }
   ],
   "source": [
    "# TODO: insert your code for experiments\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "# Load data\n",
    "ionosphere = pd.read_csv('./datasets/datasets/ionosphere.csv')\n",
    "\n",
    "# Drop features that are the same for all labels [or highly correlated with others?]\n",
    "ionosphere = ionosphere.drop(\"feature2\", axis=1) #.drop(\"feature15\", axis=1)\n",
    "\n",
    "\n",
    "# Ordinal Encoding\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "# Label encoding\n",
    "label = ionosphere[[\"label\"]]\n",
    "label_encoded = ordinal_encoder.fit_transform(label) #g=1, b=0\n",
    "\n",
    "# Shuffle \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = ionosphere.drop(\"label\", axis=1).to_numpy()\n",
    "y = label_encoded\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=1154)\n",
    "\n",
    "# Normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(Xtrain)\n",
    "Xtrain = scaler.transform(Xtrain)\n",
    "Xtest = scaler.transform(Xtest)\n",
    "feature_types=['b','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r']\n",
    "# NBC\n",
    "NBC_ionosphere = NBC(feature_types=feature_types, num_classes=2)\n",
    "#Xtrain[0].shape\n",
    "NBC_ionosphere.fit(Xtrain,ytrain)\n",
    "yhat =NBC_ionosphere.predict(Xtest)\n",
    "test_accuracy = np.mean(yhat == ytest)\n",
    "print(\"Accuracy:\", test_accuracy)\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report\n",
    "\n",
    "First, we dropped feature2, as all values of that column were 0. [We also dropped feature15, as it is highly correlated with feature13, and therefore does not add much more information in our opinion.] Also, feature1 is binary and there its datatype was changed to object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 6: Sonar Dataset**\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/connectionist+bench+%28sonar,+mines+vs.+rocks%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Empty DataFrame\n",
      "Columns: []\n",
      "Index: [], Empty DataFrame\n",
      "Columns: []\n",
      "Index: []]\n",
      "Accuracy: 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "# TODO: insert your code for experiments\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "# Load Data\n",
    "sonar = pd.read_csv('./datasets/datasets/sonar.csv')\n",
    "\n",
    "# Change label datatype to object\n",
    "#sonar[\"label\"] = sonar[\"label\"].apply(lambda x: str(x))\n",
    "\n",
    "# Shuffle \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = sonar.drop(\"label\", axis=1).to_numpy()\n",
    "y = sonar[\"label\"].to_numpy()\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=1154)\n",
    "\n",
    "# Normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(Xtrain)\n",
    "Xtrain = scaler.transform(Xtrain)\n",
    "Xtest = scaler.transform(Xtest)\n",
    "\n",
    "# NBC\n",
    "NBC_sonar = NBC(feature_types=['r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r','r'], num_classes=2)\n",
    "test= NBC_sonar.fit(Xtrain,ytrain)\n",
    "print(test)\n",
    "yhat =NBC_sonar.predict(Xtest)\n",
    "test_accuracy = np.mean(yhat == ytest)\n",
    "print(\"Accuracy:\", test_accuracy)\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Report\n",
    "\n",
    "In terms of data preparation, there was not much to do."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
